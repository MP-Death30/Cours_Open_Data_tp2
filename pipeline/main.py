#!/usr/bin/env python3
"""Script principal du pipeline."""
import argparse
from datetime import datetime
import pandas as pd
import logging # NOUVEAU
from logging.handlers import RotatingFileHandler # NOUVEAU (pour le log structur√©)
from pathlib import Path # NOUVEAU (pour les chemins de log)

from .fetchers.openmeteo import OpenMeteoFetcher, CITIES_FRANCE 
from .enricher import DataEnricher
from .transformer import DataTransformer
from .quality import QualityAnalyzer
# Import des fonctions de stockage + la nouvelle classe StorageManager (√† importer)
from .storage import save_raw_json, save_parquet, StorageManager 
from .config import MAX_ITEMS, REPORTS_DIR # Import REPORTS_DIR pour le check incr√©mental

# Configuration du logger
logger = logging.getLogger(__name__) # NOUVEAU

def setup_logging():
    """Configure le syst√®me de logging structur√© (Bonus)."""
    log_formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s - %(name)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Configuration du Handler pour la console
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_formatter)
    
    # Configuration du Handler pour le fichier (Rolling file)
    log_file = Path('logs/pipeline.log')
    log_file.parent.mkdir(exist_ok=True)
    file_handler = RotatingFileHandler(
        log_file, 
        maxBytes=1024*1024*5, # 5MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setFormatter(log_formatter)
    
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO) # Niveau par d√©faut
    
    # √âvite d'ajouter plusieurs fois les handlers lors des rechargements (si besoin)
    if not root_logger.handlers:
        root_logger.addHandler(console_handler)
        root_logger.addHandler(file_handler)


def run_pipeline(
    max_items: int = MAX_ITEMS,
    skip_enrichment: bool = False,
    verbose: bool = True
) -> dict:
    """
    Ex√©cute le pipeline complet.
    
    Args:
        max_items: Nombre max de villes √† interroger
        skip_enrichment: Passer l'enrichissement (plus rapide)
        verbose: Afficher la progression
    
    Returns:
        Statistiques du pipeline
    """
    stats = {"start_time": datetime.now()}
    
    storage_manager = StorageManager() # NOUVEAU : Instancier le manager
    
    logger.info("=" * 60)
    logger.info("üöÄ PIPELINE OPEN DATA - M√âT√âO & G√âO")
    logger.info("=" * 60)
    
    # --- LOGIQUE INCR√âMENTALE (BONUS) ---
    if storage_manager.file_exists_for_today(REPORTS_DIR, "meteo_quality"):
        logger.warning(f"Pipeline saut√© : Rapport de qualit√© d√©j√† existant pour aujourd'hui dans {REPORTS_DIR}. Ex√©cution incr√©mentale.")
        return {"status": "skipped_incremental", "end_time": datetime.now()}
    
    # === √âTAPE 1 : Acquisition ===
    city_list = CITIES_FRANCE 
    
    logger.info("\nüì• √âTAPE 1 : Acquisition des donn√©es") # LOG CHANGE
    fetcher = OpenMeteoFetcher()
    forecasts = list(fetcher.fetch_all(city_list, max_items, verbose)) 
    
    if not forecasts:
        logger.error("‚ùå Aucune pr√©vision r√©cup√©r√©e. Arr√™t.") # LOG CHANGE
        return {"error": "No data fetched"}
    
    save_raw_json(forecasts, "meteo_raw")
    stats["fetcher"] = fetcher.get_stats()
    
    # === √âTAPE 2 : Enrichissement ===
    if not skip_enrichment:
        logger.info("\nüåç √âTAPE 2 : Enrichissement (g√©ocodage)") # LOG CHANGE
        enricher = DataEnricher()
        
        # Extraire les noms de villes uniques
        addresses = enricher.extract_addresses(forecasts, "original_city_name")
        
        if addresses:
            # Construire le cache de g√©ocodage
            geo_cache = enricher.build_geocoding_cache(addresses) 
            
            # Enrichir les pr√©visions
            forecasts = enricher.enrich_forecasts(forecasts, geo_cache, "original_city_name") 
            stats["enricher"] = enricher.get_stats()
        else:
            logger.warning("‚ö†Ô∏è Pas de villes √† g√©ocoder") # LOG CHANGE
    else:
        logger.info("\n‚è≠Ô∏è √âTAPE 2 : Enrichissement (ignor√©)") # LOG CHANGE
    
    # === √âTAPE 3 : Transformation ===
    logger.info("\nüîß √âTAPE 3 : Transformation et nettoyage") # LOG CHANGE
    df = pd.DataFrame(forecasts)
    
    transformer = DataTransformer(df)
    df_clean = (
        transformer
        .remove_duplicates(['date', 'latitude', 'longitude']) 
        .handle_missing_values(numeric_strategy='median', text_strategy='unknown')
        .normalize_text_columns(['validated_city'])
        .add_derived_columns()
        .get_result()
    )
    
    logger.info(f"   R√©sum√© des transformations:\n{transformer.get_summary()}") # LOG CHANGE
    stats["transformer"] = {"transformations": transformer.transformations_applied}
    
    # === √âTAPE 4 : Qualit√© ===
    logger.info("\nüìä √âTAPE 4 : Analyse de qualit√©") # LOG CHANGE
    analyzer = QualityAnalyzer(df_clean)
    metrics = analyzer.analyze()
    
    logger.info(f"   Note: {metrics.quality_grade}") # LOG CHANGE
    logger.info(f"   Compl√©tude: {metrics.completeness_score * 100:.1f}%") # LOG CHANGE
    logger.info(f"   Doublons: {metrics.duplicates_pct:.1f}%") # LOG CHANGE
    
    # G√©n√©rer le rapport
    analyzer.generate_report("meteo_quality")
    stats["quality"] = metrics.dict()
    
    # === √âTAPE 5 : Stockage ===
    logger.info("\nüíæ √âTAPE 5 : Stockage final") # LOG CHANGE
    output_path = save_parquet(df_clean, "meteo_enriched") 
    stats["output_path"] = str(output_path)
    
    # === R√âSUM√â ===
    stats["end_time"] = datetime.now()
    stats["duration_seconds"] = (stats["end_time"] - stats["start_time"]).seconds
    
    logger.info("\n" + "=" * 60) # LOG CHANGE
    logger.info("‚úÖ PIPELINE TERMIN√â") # LOG CHANGE
    logger.info("=" * 60) # LOG CHANGE
    logger.info(f"   Dur√©e: {stats['duration_seconds']}s") # LOG CHANGE
    logger.info(f"   Enregistrements: {len(df_clean)}") # LOG CHANGE
    logger.info(f"   Qualit√©: {metrics.quality_grade}") # LOG CHANGE
    logger.info(f"   Fichier: {output_path}") # LOG CHANGE
    
    return stats


def main():
    # NOUVEAU : Configure le logging en premier
    setup_logging()
    
    parser = argparse.ArgumentParser(description="Pipeline Open Data M√©t√©o")
    parser.add_argument("--max-items", "-m", type=int, default=MAX_ITEMS, help="Nombre max de villes")
    parser.add_argument("--skip-enrichment", "-s", action="store_true", help="Ignorer l'enrichissement")
    parser.add_argument("--verbose", "-v", action="store_true", default=True)
    
    args = parser.parse_args()
    
    run_pipeline(
        max_items=args.max_items,
        skip_enrichment=args.skip_enrichment,
        verbose=args.verbose
    )


if __name__ == "__main__":
    main()