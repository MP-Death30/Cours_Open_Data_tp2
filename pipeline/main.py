#!/usr/bin/env python3
"""Script principal du pipeline."""
import argparse
from datetime import datetime
import pandas as pd

from .fetchers.openmeteo import OpenMeteoFetcher, CITIES_FRANCE # Utiliser le nouveau fetcher
from .enricher import DataEnricher
from .transformer import DataTransformer
from .quality import QualityAnalyzer
from .storage import save_raw_json, save_parquet
from .config import MAX_ITEMS


def run_pipeline(
    max_items: int = MAX_ITEMS,
    skip_enrichment: bool = False,
    verbose: bool = True
) -> dict:
    """
    Ex√©cute le pipeline complet.
    
    Args:
        max_items: Nombre max de villes √† interroger
        skip_enrichment: Passer l'enrichissement (plus rapide)
        verbose: Afficher la progression
    
    Returns:
        Statistiques du pipeline
    """
    stats = {"start_time": datetime.now()}
    
    print("=" * 60)
    print("üöÄ PIPELINE OPEN DATA - M√âT√âO & G√âO")
    print("=" * 60)
    
    # === √âTAPE 1 : Acquisition ===
    # Ici, la "cat√©gorie" est la liste de villes √† interroger
    city_list = CITIES_FRANCE 
    
    print("\nüì• √âTAPE 1 : Acquisition des donn√©es")
    fetcher = OpenMeteoFetcher()
    # R√©cup√®re les pr√©visions pour MAX_ITEMS villes
    forecasts = list(fetcher.fetch_all(city_list, max_items, verbose)) 
    
    if not forecasts:
        print("‚ùå Aucune pr√©vision r√©cup√©r√©e. Arr√™t.")
        return {"error": "No data fetched"}
    
    save_raw_json(forecasts, "meteo_raw")
    stats["fetcher"] = fetcher.get_stats()
    
    # === √âTAPE 2 : Enrichissement ===
    if not skip_enrichment:
        print("\nüåç √âTAPE 2 : Enrichissement (g√©ocodage)")
        enricher = DataEnricher()
        
        # Extraire les noms de villes uniques
        addresses = enricher.extract_addresses(forecasts, "original_city_name")
        
        if addresses:
            # Construire le cache de g√©ocodage
            # On utilise toutes les adresses, car MAX_ITEMS est d√©j√† bas
            geo_cache = enricher.build_geocoding_cache(addresses) 
            
            # Enrichir les pr√©visions
            # Utiliser la m√©thode adapt√©e pour les pr√©visions
            forecasts = enricher.enrich_forecasts(forecasts, geo_cache, "original_city_name") 
            stats["enricher"] = enricher.get_stats()
        else:
            print("‚ö†Ô∏è Pas de villes √† g√©ocoder")
    else:
        print("\n‚è≠Ô∏è √âTAPE 2 : Enrichissement (ignor√©)")
    
    # === √âTAPE 3 : Transformation ===
    print("\nüîß √âTAPE 3 : Transformation et nettoyage")
    df = pd.DataFrame(forecasts)
    
    transformer = DataTransformer(df)
    # Les transformations sont g√©n√©riques, mais on pourrait ajouter le nettoyage des codes WMO
    df_clean = (
        transformer
        .remove_duplicates(['date', 'latitude', 'longitude']) 
        .handle_missing_values(numeric_strategy='median', text_strategy='unknown')
        .normalize_text_columns(['validated_city']) # Utiliser validated_city pour la normalisation
        .add_derived_columns()
        .get_result()
    )
    
    print(f"   R√©sum√© des transformations:\n{transformer.get_summary()}")
    stats["transformer"] = {"transformations": transformer.transformations_applied}
    
    # === √âTAPE 4 : Qualit√© ===
    print("\nüìä √âTAPE 4 : Analyse de qualit√©")
    analyzer = QualityAnalyzer(df_clean)
    metrics = analyzer.analyze()
    
    print(f"   Note: {metrics.quality_grade}")
    print(f"   Compl√©tude: {metrics.completeness_score * 100:.1f}%")
    print(f"   Doublons: {metrics.duplicates_pct:.1f}%")
    
    # G√©n√©rer le rapport
    analyzer.generate_report("meteo_quality")
    stats["quality"] = metrics.dict()
    
    # === √âTAPE 5 : Stockage ===
    print("\nüíæ √âTAPE 5 : Stockage final")
    # Nom du fichier par d√©faut
    output_path = save_parquet(df_clean, "meteo_enriched") 
    stats["output_path"] = str(output_path)
    
    # === R√âSUM√â ===
    stats["end_time"] = datetime.now()
    stats["duration_seconds"] = (stats["end_time"] - stats["start_time"]).seconds
    
    print("\n" + "=" * 60)
    print("‚úÖ PIPELINE TERMIN√â")
    print("=" * 60)
    print(f"   Dur√©e: {stats['duration_seconds']}s")
    print(f"   Enregistrements: {len(df_clean)}")
    print(f"   Qualit√©: {metrics.quality_grade}")
    print(f"   Fichier: {output_path}")
    
    return stats


def main():
    parser = argparse.ArgumentParser(description="Pipeline Open Data M√©t√©o")
    # Nous avons retir√© l'argument 'category' car la liste est fixe (CITIES_FRANCE)
    parser.add_argument("--max-items", "-m", type=int, default=MAX_ITEMS, help="Nombre max de villes")
    parser.add_argument("--skip-enrichment", "-s", action="store_true", help="Ignorer l'enrichissement")
    parser.add_argument("--verbose", "-v", action="store_true", default=True)
    
    args = parser.parse_args()
    
    run_pipeline(
        max_items=args.max_items,
        skip_enrichment=args.skip_enrichment,
        verbose=args.verbose
    )


if __name__ == "__main__":
    main()